{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4cf311b-ae9f-442b-80c6-1ecfd8128635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\Desktop\\startup\\vbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4818c5-cf9e-4644-9bab-2e45714bc20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your CSV is inside a folder named \"data\"\n",
    "df = pd.read_csv(\"data/conversational_data.csv\", encoding='utf-8')  # 👈 replace with your actual path\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2858a979-7712-459b-9b61-928728216d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi, how are you doing?</td>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i've been great. what about you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>i've been good. i'm in school right now.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 input  \\\n",
       "0               hi, how are you doing?   \n",
       "1        i'm fine. how about yourself?   \n",
       "2  i'm pretty good. thanks for asking.   \n",
       "3    no problem. so how have you been?   \n",
       "4     i've been great. what about you?   \n",
       "\n",
       "                                     target  \n",
       "0             i'm fine. how about yourself?  \n",
       "1       i'm pretty good. thanks for asking.  \n",
       "2         no problem. so how have you been?  \n",
       "3          i've been great. what about you?  \n",
       "4  i've been good. i'm in school right now.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2023da4c-f4ce-4de5-8751-962f551030b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\Desktop\\startup\\vbot\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/gpt2_tokenizer/tokenizer_config.json',\n",
       " 'models/gpt2_tokenizer/special_tokens_map.json',\n",
       " 'models/gpt2_tokenizer/vocab.json',\n",
       " 'models/gpt2_tokenizer/merges.txt',\n",
       " 'models/gpt2_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # ✅ Important for GPT2 which doesn't have PAD token\n",
    "tokenizer.save_pretrained(\"models/gpt2_tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab6b1fd-0668-486c-9ac9-9d014b60aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.data.iloc[idx]['input']\n",
    "        target_text = self.data.iloc[idx]['target']\n",
    "\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': input_encoding['attention_mask'].squeeze(),\n",
    "            'labels': target_encoding['input_ids'].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae5b566a-598a-4174-a0fe-b9252a0c8717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset object\n",
    "dataset = ChatDataset(df, tokenizer, max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "156311bf-6254-49e6-9e6c-2c5468517563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully dump ho gya\n"
     ]
    }
   ],
   "source": [
    "# Now save the dataset to a pickle file\n",
    "import pickle\n",
    "\n",
    "with open(\"data/processed_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset, f)\n",
    "    print(\"successfully dump ho gya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5122bcee-9095-4531-bbc2-68a7353ecc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd37030-303a-47ec-bc5c-4f7b27997dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c85eb76a-b825-493c-b1cc-bc94947ba81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from transformers import GPT2Tokenizer\n",
    "# import torch\n",
    "# import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06128a6f-d316-446b-b335-cddf3d510bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make sure your CSV is inside a folder named \"data\"\n",
    "# df = pd.read_csv(\"data/conversational_data.csv\", encoding='utf-8')  # 👈 replace with your actual path\n",
    "# df.dropna(inplace=True)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Show sample\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2083c4-a5c9-4054-9c5d-2a1749e7e89c",
   "metadata": {},
   "source": [
    "#### Initialize GPT-2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb380b17-2028-457b-8345-ba4d4f2eb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token  # ✅ Important for GPT2 which doesn't have PAD token\n",
    "# tokenizer.save_pretrained(\"models/gpt2_tokenizer/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd27b5-8f8d-4194-850f-40aa3ece80e1",
   "metadata": {},
   "source": [
    "#### Tokenize input & target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01ca270c-acca-4df4-b7a5-76f4436ea7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class ChatDataset(Dataset):\n",
    "#     def __init__(self, dataframe, tokenizer, max_length=64):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.data = dataframe\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         input_text = self.data.iloc[idx]['input']\n",
    "#         target_text = self.data.iloc[idx]['target']\n",
    "\n",
    "#         input_encoding = self.tokenizer(\n",
    "#             input_text,\n",
    "#             truncation=True,\n",
    "#             padding='max_length',\n",
    "#             max_length=self.max_length,\n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "\n",
    "#         target_encoding = self.tokenizer(\n",
    "#             target_text,\n",
    "#             truncation=True,\n",
    "#             padding='max_length',\n",
    "#             max_length=self.max_length,\n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': input_encoding['input_ids'].squeeze(),\n",
    "#             'attention_mask': input_encoding['attention_mask'].squeeze(),\n",
    "#             'labels': target_encoding['input_ids'].squeeze()\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166f6a6-00e8-4190-9c6f-371ddb0cd205",
   "metadata": {},
   "source": [
    "#### Stack & Save Tensor Da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d719ee6-920a-4cdf-ac35-13eabd0f1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"data/processed_dataset.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(dataset, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84deb34e-63da-456f-8794-6de0c0a632f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "cff21c51-5d33-40e8-bbb9-d7226211fba8",
   "metadata": {},
   "source": [
    "#📁 your_project/\n",
    "├── 01_prepare_data.ipynb ✅\n",
    "├── data/\n",
    "│   ├── conversations.csv ✅\n",
    "│   └── tokenized_dataset.pt ✅\n",
    "├── models/\n",
    "│   └── gpt2_tokenizer/ ✅\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a8668-8052-46e7-8a84-50e0d947f7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9e2888c-de87-40d8-97f1-f0717f8592e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Function to tokenize the dataset\n",
    "# def tokenize_data(data):\n",
    "#     input_texts = data['input'].tolist()  # 'input' column\n",
    "#     target_texts = data['target'].tolist()  # 'output' column\n",
    "    \n",
    "#     # Tokenize both input and output texts\n",
    "#     input_ids = tokenizer(input_texts, truncation=True, padding=True, max_length=50, return_tensors=\"pt\")\n",
    "#     target_ids = tokenizer(target_texts, truncation=True, padding=True, max_length=50, return_tensors=\"pt\")\n",
    "    \n",
    "#     return input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72331c22-5aa3-49b5-9d3b-5c1ad57facc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize the data\n",
    "# input_ids, target_ids = tokenize_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5839e181-d64a-4dfe-b231-b98cf5b3cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the tokenized outputs\n",
    "# print(input_ids)\n",
    "# print(target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c94ac416-2a3a-438e-ba40-0fc66f20bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Save tokenizer\n",
    "# tokenizer.save_pretrained(\"models/gpt2_tokenizer/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vbot",
   "language": "python",
   "name": "vbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
