{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09dc4dce-0cd1-4abe-9061-d63255a05b43",
   "metadata": {},
   "source": [
    "#### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db135f3c-55f4-4f3e-b5d0-a6db8339ceb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & Tokenizer loaded ✅\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load model & tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"models/fine_tuned_gpt2/\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"models/gpt2_tokenizer/\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model & Tokenizer loaded ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16fa408-6600-4d98-a829-54d51bc0ea7f",
   "metadata": {},
   "source": [
    "#### Step 2: Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a90a5b-eaf4-4ffa-ba32-4d29a945f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reply(user_input, max_length=60):\n",
    "    prompt = f\"<|startoftext|>User: {user_input}\\nBot:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=len(inputs[\"input_ids\"][0]) + max_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.8,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode and clean up\n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract reply after \"Bot:\"\n",
    "    if \"Bot:\" in full_text:\n",
    "        reply = full_text.split(\"Bot:\")[-1].strip()\n",
    "    else:\n",
    "        reply = full_text.strip()\n",
    "\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d649f4b6-117b-43cd-a54f-0a9138e83bdf",
   "metadata": {},
   "source": [
    "#### Step 3: Chat Loop (Try it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "325f8657-f443-488a-bf9b-0e1a3d5758a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  kese ho\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:   hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in ['exit', 'quit']:\n",
    "        break\n",
    "    bot_reply = generate_reply(user_input)\n",
    "    print(\"Bot:\", bot_reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "087f263a-ac33-47f9-a7b9-9201e9e2f806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  kese ho\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: [No response generated]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  kya haal hai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: !\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: [No response generated]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  good morning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: [No response generated]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  kese ho\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"models/gpt2_tokenizer/\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"models/fine_tuned_gpt2/\")\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Inference function\n",
    "def generate_response(prompt, max_length=100):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Remove the prompt from the response to keep only bot reply\n",
    "    bot_reply = response[len(prompt):].strip()\n",
    "    return bot_reply if bot_reply else \"[No response generated]\"\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    response = generate_response(user_input)\n",
    "    print(f\"Bot: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e524b-0180-4a25-9129-847821a3696e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vbot",
   "language": "python",
   "name": "vbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
